{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 2. Artificial Neural Networks\n\n<p style=\"text-align: justify;\">Artificial Neural Networks are mathematical models inspired by the human brain, specifically the ability to learn, process, and perform tasks. The Artificial Neural Networks are powerful tools that assist in solving complex problems linked mainly in the area of combinatorial optimization and machine learning. In this context, artificial neural networks have the most varied applications possible, as such models can adapt to the situations presented, ensuring a gradual increase in performance without any human interference. We can say that the Artificial Neural Networks are potent methods can give computers a new possibility, that is, a machine does not get stuck to preprogrammed rules and opens up various options to learn from its own mistakes. </p>\n\n<img src=\"https://groupfuturista.com/blog/wp-content/uploads/2019/03/Artificial-Neural-Networks-Man-vs-Machine.jpeg\">"},{"metadata":{},"cell_type":"markdown","source":"# 3. How implement a Multilayer Perceptron"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Some Python Libraries \n\n<p style=\"text-align: justify;\">In the first place, Let's define some libraries to help us in the manipulation the data set, such as `numpy`, `matplotlib`, `seaborn` and `scikit-learn`. In this tutorial, I am implementing a Multilayer Perceptron without any framework like Keras or similar ones. The goal here is to be as simple as possible! So to help you with this task, we implementing the neural network without using ready-made libraries. You can use numpy to work with array operations! There is no problem it! </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport random\nimport seaborn\n\nseaborn.set(style='whitegrid'); seaborn.set_context('talk')\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom sklearn.datasets import load_iris\niris_data = load_iris()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. An analysis about the Iris Flower Dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(iris_data['DESCR'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples, n_features = iris_data.data.shape\n\nplt.subplot(1, 2, 1)\nscatter_plot = plt.scatter(iris_data.data[:,0], iris_data.data[:,1], alpha=0.5, \n                           c=iris_data.target) \nplt.colorbar(ticks=([0, 1, 2]))\nplt.title('Sepal Sample')\n\nplt.subplot(1, 2, 2)\nscatter_plot_2 = plt.scatter(iris_data.data[:,2], iris_data.data[:,3], alpha=0.5, \n                           c=iris_data.target)\nplt.colorbar(ticks=([0, 1, 2]))\nplt.title('Petal Sample')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\nfrom pandas.plotting import scatter_matrix\n\n\ndataset = pandas.read_csv('../input/iris/Iris.csv')\nscatter_matrix(dataset, alpha=0.5, figsize=(20, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.hist(alpha=0.5, figsize=(20, 20), color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.plot(subplots=True, figsize=(10, 10), sharex=False, sharey=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Manually separating our dataset\n\nIt is here that we will select our samples to train and test the algorithms: **80% Training Samples and 20% Test**\n<div class=\"container-fluid\">\n  <div class=\"row\">\n      <div class=\"col-md-2\" align='center'>\n      </div>\n      <div class='col-md-8' align='center'>\n      </div>\n      <div class=\"col-md-2\" align='center'></div>\n  </div>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(123)\n\ndef separate_data():\n    A = iris_dataset[0:40]\n    tA = iris_dataset[40:50]\n    B = iris_dataset[50:90]\n    tB = iris_dataset[90:100]\n    C = iris_dataset[100:140]\n    tC = iris_dataset[140:150]\n    train = np.concatenate((A,B,C))\n    test =  np.concatenate((tA,tB,tC))\n    return train,test\n\ntrain_porcent = 80 # Porcent Training \ntest_porcent = 20 # Porcent Test\niris_dataset = np.column_stack((iris_data.data,iris_data.target.T)) #Join X and Y\niris_dataset = list(iris_dataset)\nrandom.shuffle(iris_dataset)\n\nFiletrain, Filetest = separate_data()\n\ntrain_X = np.array([i[:4] for i in Filetrain])\ntrain_y = np.array([i[4] for i in Filetrain])\ntest_X = np.array([i[:4] for i in Filetest])\ntest_y = np.array([i[4] for i in Filetest])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Plot our training Samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\n\nplt.subplot(1, 2, 1)\nplt.scatter(train_X[:,0],train_X[:,1],c=train_y,cmap=cm.viridis)\nplt.xlabel(iris_data.feature_names[0])\nplt.ylabel(iris_data.feature_names[1])\n\nplt.subplot(1, 2, 2)\nplt.scatter(train_X[:,2],train_X[:,3],c=train_y,cmap=cm.viridis)\nplt.xlabel(iris_data.feature_names[2])\nplt.ylabel(iris_data.feature_names[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Plot our test Samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(1, 2, 1)\nplt.scatter(test_X[:,0],test_X[:,1],c=test_y,cmap=cm.viridis)\nplt.xlabel(iris_data.feature_names[0])\nplt.ylabel(iris_data.feature_names[1]) \n\nplt.subplot(1, 2, 2)\nplt.scatter(test_X[:,2],test_X[:,3],c=test_y,cmap=cm.viridis)\nplt.xlabel(iris_data.feature_names[2])\nplt.ylabel(iris_data.feature_names[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n# 5. Multilayer Perceptron\n\n<p style=\"text-align: justify;\">Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, \"they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images\".</p>\n\n<p style=\"text-align: justify;\">They have found most use in applications difficult to express in a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to axons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.</p>\n\n<p style=\"text-align: justify;\"> More information here: [Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)</p>\n\n<img src=\"https://miro.medium.com/max/1072/1*DOkHU_dgXMCybA6WWXrp4g.gif\"/>\n\n<p style=\"text-align: justify;\">The Multilayer Perceptron Networks are characterized by the presence of many intermediate layers (hidden) in your structure, located between input layer and output layer. With this, such networks have the advantage of being able to classify more than two different classes and It also solve non-linearly separable problems.</p>\n<div class=\"container-fluid\"><div class=\"row\">\n      <div class=\"col-md-2\" align='center'></div>\n      <div class='col-md-8' align='center'>\n           <img src='http://ffden-2.phys.uaf.edu/212_fall2003.web.dir/Keith_Palchikoff/multilayer%20perceptron.JPG' />\n      </div><div class=\"col-md-2\" align='center'></div>\n  </div>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"## 5.1. How does Multilayer Perceptron work? \n\n<p style=\"text-align: justify;\"> We can summarize the operation of the perceptron as follows it:</p>\n\n  - **Step 1**: Initialize the weights and bias with small-randomized values;\n  - **Step 2**: Propagate all values in the input layer until output layer(Forward Propagation)\n  - **Step 3**: Update weight and bias in the inner layers(Backpropagation)\n  - **Step 4**: Do it until that the stop criterion is satisfied !\n  \n### Step 1: Forward propagation Algorithm\n<img src=\"https://sebastianraschka.com/images/faq/visual-backpropagation/forward-propagation.png\">\n\nIn order to proceed we need to improve the notation we have been using. That for, for each layer $1\\geq l\\geq L$, the activations and outputs are calculated as:\n\n$$\n\\text{L}^l_j = {\\sum_i w^l_{ji} x^l_i\\, = w^l_{j,0} x^l_0 + w^l_{j,1} x^l_1 + w^l_{j,2} x^l_2 + ... + w^l_{j,n}} x^l_n,\n$$\n$$Y^l_j = g^l(\\text{L}^l_j)\\,,$$\n\n$$\\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}$$\n\nwhere:\n\n* $y^l_j$ is the $j-$th output of layer $l$,\n* $x^l_i$ is the $i$-th input to layer $l$,\n* $w^l_{ji}$ is the weight of the $j$-th neuron connected to input $i$,\n* $\\text{L}^l_{j}$ is called net activation, and\n* $g^l(\\cdot)$ is the activation function of layer $l$."},{"metadata":{},"cell_type":"markdown","source":"### Step 2: Calculation our Erro function \n<img src=\"https://miro.medium.com/max/920/1*jYQYuHpHdkZqNFQKJSuDTw.png\">\nIt is used to measure performance locality associated with the results produced by the neurons in output layer and the expected result.\n$$\nE(k) = \n\\frac{1}{2} \\sum_{k=1}^{K}({{d_j(k)}} - {y_j}{(k)})^2.\n$$"},{"metadata":{},"cell_type":"markdown","source":"### Step 3. Activation Functions\n<img src=\"https://miro.medium.com/max/1192/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\">\n ### Sigmoid Function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = 0 #example value\nativation = {(lambda x: 1/(1 + np.exp(-x)))}\nderiv = {(lambda x: x*(1-x))}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Hyperbolic Tangent Function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"activation_tang = {(lambda x: np.tanh(x))}\nderiv_tang = {(lambda x: 1-x**2)}\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ReLU Function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"activation_ReLU = {(lambda x: x*(x > 0))}\nderiv_ReLU = {(lambda x: 1 * (x>0))}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### Step 4. Backpropagation Algorithm\n<img src=\"https://sebastianraschka.com/images/faq/visual-backpropagation/backpropagation.png\">\n### In Output Layer,  $L = 2:$\n   - **Step 1**: Calculate error in output layer: $\\delta^{(L2)} = -({d_j}^{(L2)} - {y_j}^{(L2)})\\cdot\n   g'({S_j}^{(L2)})$\n   \n      `\n      ERROR_output = self.OUTPUT - self.OUTPUT_L2\n      DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n      `\n      \n\n   - **Step 2**: Update all weight between hidden and output layer: $W^{(L2)} = W^{(L2)} -\\gamma \\cdot(\\delta^{(L2)}  - {S_j}^{(L1)})$\n   \n         for i in range(self.hiddenLayer):`\n           ` for j in range(self.OutputLayer):`\n               ` self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.output_l1[i]))`\n               ` self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])`\n               \n   - **Step 3**: Update bias value in output layer: $bias^{(L2)} = bias^{(L2)} - \\gamma \\cdot \\delta^{(L2)}$\n   \n### In Input Layer , $L = 1$:\n   - **Step 4**: Calculate error in hidden layer: $\\delta^{(L1)} = W^{(L2)} \\cdot \\delta^{(L2)} \\cdot g'({S_j}^{(L1)})$\n     \n   `delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output) * self.deriv(self._l1)`\n   - **Step 5**: Update all weight between hidden and output layer: $W^{(L1)} = W^{(L1)} -\\gamma \\cdot(\\delta^{(L1)}  - {X_i})$\n         `for i in range(self.OutputLayer):`\n           `for j in range(self.hiddenLayer):`\n               `self.WEIGHT_hidden[i][j] -= (self.learningRate * (DELTA_hidden[j] * INPUT[i]))`\n               `self.BIAS_hidden[j] -= (self.learningRate * DELTA_hidden[j])`\n   - **Step 6**: Update bias value in output layer: $bias^{(L1)} = bias^{(L1)} - \\gamma \\cdot \\delta^{(L1)}$"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://thumbs.gfycat.com/FickleHorribleBlackfootedferret-small.gif\">"},{"metadata":{},"cell_type":"markdown","source":"# 6. Implementation the Multilayer Perceptron in Python\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\nimport random\n\nclass MultiLayerPerceptron(BaseEstimator, ClassifierMixin): \n    def __init__(self, params=None):     \n        if (params == None):\n            self.inputLayer = 4                        # Input Layer\n            self.hiddenLayer = 5                       # Hidden Layer\n            self.outputLayer = 3                       # Outpuy Layer\n            self.learningRate = 0.005                  # Learning rate\n            self.max_epochs = 600                      # Epochs\n            self.iasHiddenValue = -1                   # Bias HiddenLayer\n            self.BiasOutputValue = -1                  # Bias OutputLayer\n            self.activation = self.ativacao['sigmoid'] # Activation function\n            self.deriv = self.derivada['sigmoid']\n        else:\n            self.inputLayer = params['InputLayer']\n            self.hiddenLayer = params['HiddenLayer']\n            self.OutputLayer = params['OutputLayer']\n            self.learningRate = params['LearningRate']\n            self.max_epochs = params['Epocas']\n            self.BiasHiddenValue = params['BiasHiddenValue']\n            self.BiasOutputValue = params['BiasOutputValue']\n            self.activation = self.ativacao[params['ActivationFunction']]\n            self.deriv = self.derivada[params['ActivationFunction']]\n        \n        'Starting Bias and Weights'\n        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n        self.WEIGHT_output = self.starting_weights(self.OutputLayer, self.hiddenLayer)\n        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])\n        self.classes_number = 3 \n        \n    pass\n    \n    def starting_weights(self, x, y):\n        return [[2  * random.random() - 1 for i in range(x)] for j in range(y)]\n\n    ativacao = {\n         'sigmoid': (lambda x: 1/(1 + np.exp(-x))),\n            'tanh': (lambda x: np.tanh(x)),\n            'Relu': (lambda x: x*(x > 0)),\n               }\n    derivada = {\n         'sigmoid': (lambda x: x*(1-x)),\n            'tanh': (lambda x: 1-x**2),\n            'Relu': (lambda x: 1 * (x>0))\n               }\n \n    def Backpropagation_Algorithm(self, x):\n        DELTA_output = []\n        'Stage 1 - Error: OutputLayer'\n        ERROR_output = self.output - self.OUTPUT_L2\n        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n        \n        arrayStore = []\n        'Stage 2 - Update weights OutputLayer and HiddenLayer'\n        for i in range(self.hiddenLayer):\n            for j in range(self.OutputLayer):\n                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))\n                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n      \n        'Stage 3 - Error: HiddenLayer'\n        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)\n \n        'Stage 4 - Update weights HiddenLayer and InputLayer(x)'\n        for i in range(self.OutputLayer):\n            for j in range(self.hiddenLayer):\n                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))\n                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n                \n    def show_err_graphic(self,v_erro,v_epoca):\n        plt.figure(figsize=(9,4))\n        plt.plot(v_epoca, v_erro, \"m-\",color=\"b\", marker=11)\n        plt.xlabel(\"Number of Epochs\")\n        plt.ylabel(\"Squared error (MSE) \");\n        plt.title(\"Error Minimization\")\n        plt.show()\n\n    def predict(self, X, y):\n        'Returns the predictions for every element of X'\n        my_predictions = []\n        'Forward Propagation'\n        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden\n        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output\n                                 \n        for i in forward:\n            my_predictions.append(max(enumerate(i), key=lambda x:x[1])[0])\n            \n        print(\" Number of Sample  | Class |  Output |  Hoped Output  \")   \n        for i in range(len(my_predictions)):\n            if(my_predictions[i] == 0): \n                print(\"id:{}    | Iris-Setosa  |  Output: {}  \".format(i, my_predictions[i], y[i]))\n            elif(my_predictions[i] == 1): \n                print(\"id:{}    | Iris-Versicolour    |  Output: {}  \".format(i, my_predictions[i], y[i]))\n            elif(my_predictions[i] == 2): \n                print(\"id:{}    | Iris-Iris-Virginica   |  Output: {}  \".format(i, my_predictions[i], y[i]))\n                \n        return my_predictions\n        pass\n\n    def fit(self, X, y):  \n        count_epoch = 1\n        total_error = 0\n        n = len(X); \n        epoch_array = []\n        error_array = []\n        W0 = []\n        W1 = []\n        while(count_epoch <= self.max_epochs):\n            for idx,inputs in enumerate(X): \n                self.output = np.zeros(self.classes_number)\n                'Stage 1 - (Forward Propagation)'\n                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n                self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n                'Stage 2 - One-Hot-Encoding'\n                if(y[idx] == 0): \n                    self.output = np.array([1,0,0]) #Class1 {1,0,0}\n                elif(y[idx] == 1):\n                    self.output = np.array([0,1,0]) #Class2 {0,1,0}\n                elif(y[idx] == 2):\n                    self.output = np.array([0,0,1]) #Class3 {0,0,1}\n                \n                square_error = 0\n                for i in range(self.OutputLayer):\n                    erro = (self.output[i] - self.OUTPUT_L2[i])**2\n                    square_error = (square_error + (0.05 * erro))\n                    total_error = total_error + square_error\n         \n                'Backpropagation : Update Weights'\n                self.Backpropagation_Algorithm(inputs)\n                \n            total_error = (total_error / n)\n            if((count_epoch % 50 == 0)or(count_epoch == 1)):\n                print(\"Epoch \", count_epoch, \"- Total Error: \",total_error)\n                error_array.append(total_error)\n                epoch_array.append(count_epoch)\n                \n            W0.append(self.WEIGHT_hidden)\n            W1.append(self.WEIGHT_output)\n             \n                \n            count_epoch += 1\n        self.show_err_graphic(error_array,epoch_array)\n        \n        plt.plot(W0[0])\n        plt.title('Weight Hidden update during training')\n        plt.legend(['neuron1', 'neuron2', 'neuron3', 'neuron4', 'neuron5'])\n        plt.ylabel('Value Weight')\n        plt.show()\n        \n        plt.plot(W1[0])\n        plt.title('Weight Output update during training')\n        plt.legend(['neuron1', 'neuron2', 'neuron3'])\n        plt.ylabel('Value Weight')\n        plt.show()\n\n        return self","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the best parameters \n\n<p style=\"text-align: justify;\">For find the best parameters, it was necessary to realize various tests using different values to the parameters. The graphs below denote all tests made to select the best configuration for the multilayer perceptron. These tests were important in selecting the best settings and ensuring the best accuracy. The graph was drawn manually, but you can change the settings and note the results obtained. The tests involve different activation functions and the number of neurons for each layer.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_test():\n    ep1 = [0,100,200,300,400,500,600,700,800,900,1000,1500,2000]\n    h_5 = [0,60,70,70,83.3,93.3,96.7,86.7,86.7,76.7,73.3,66.7,66.7]\n    h_4 = [0,40,70,63.3,66.7,70,70,70,70,66.7,66.7,43.3,33.3]\n    h_3 = [0,46.7,76.7,80,76.7,76.7,76.6,73.3,73.3,73.3,73.3,76.7,76.7]\n    plt.figure(figsize=(10,4))\n    l1, = plt.plot(ep1, h_3, \"m-\",color='b',label=\"node-3\", marker=11)\n    l2, = plt.plot(ep1, h_4, \"m-\",color='g',label=\"node-4\", marker=8)\n    l3, = plt.plot(ep1, h_5, \"m-\",color='r',label=\"node-5\", marker=5)\n    plt.legend(handles=[l1,l2,l3], loc=1)\n    plt.xlabel(\"number of Epochs\");plt.ylabel(\"% Hits\");\n    plt.title(\"Number of Hidden Layers - Performance\")\n    \n    ep2 = [0,100,200,300,400,500,600,700]\n    tanh = [0.18,0.027,0.025,0.022,0.0068,0.0060,0.0057,0.00561]\n    sigm = [0.185,0.0897,0.060,0.0396,0.0343,0.0314,0.0296,0.0281]\n    Relu = [0.185,0.05141,0.05130,0.05127,0.05124,0.05123,0.05122,0.05121]\n    plt.figure(figsize=(10,4))\n    l1 , = plt.plot(ep2, tanh, \"m-\",color='b',label=\"Hyperbolic Tangent\",marker=11)\n    l2 , = plt.plot(ep2, sigm, \"m-\",color='g',label=\"Sigmoide\", marker=8)\n    l3 , = plt.plot(ep2, Relu, \"m-\",color='r',label=\"ReLu\", marker=5)\n    plt.legend(handles=[l1,l2,l3], loc=1)\n    plt.xlabel(\"Epoch\");plt.ylabel(\"Error\");plt.title(\"Activation Functions - Performance\")\n    \n    fig, ax = plt.subplots()\n    names = [\"Hyperbolic Tangent\",\"Sigmoide\",\"ReLU\"]\n    x1 = [2.0,4.0,6.0]\n    plt.bar(x1[0],53.4,0.4,color='b')\n    plt.bar(x1[1],96.7,0.4,color='g')\n    plt.bar(x1[2],33.2,0.4,color='r')\n    plt.xticks(x1,names)\n    plt.ylabel('% Hits')\n    plt.title('Hits - Activation Functions')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_test()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Artificial Neural Network(MLP)\n\n## Step 1: training our MultiLayer Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = {'InputLayer':4, 'HiddenLayer':5, 'OutputLayer':3,\n              'Epocas':700, 'LearningRate':0.005,'BiasHiddenValue':-1, \n              'BiasOutputValue':-1, 'ActivationFunction':'sigmoid'}\n\nPerceptron = MultiLayerPerceptron(dictionary)\nPerceptron.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: testing our results "},{"metadata":{"trusted":true},"cell_type":"code","source":"prev = Perceptron.predict(test_X,test_y)\nhits = n_set = n_vers = n_virg = 0\nscore_set = score_vers = score_virg = 0\nfor j in range(len(test_y)):\n    if(test_y[j] == 0): n_set += 1\n    elif(test_y[j] == 1): n_vers += 1\n    elif(test_y[j] == 2): n_virg += 1\n        \nfor i in range(len(test_y)):\n    if test_y[i] == prev[i]: \n        hits += 1\n    if test_y[i] == prev[i] and test_y[i] == 0:\n        score_set += 1\n    elif test_y[i] == prev[i] and test_y[i] == 1:\n        score_vers += 1\n    elif test_y[i] == prev[i] and test_y[i] == 2:\n        score_virg += 1    \n         \nhits = (hits / len(test_y))*100\nfaults = 100 - hits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3. Accuracy and precision the Multilayer Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_hits = []\nprint(\"Porcents :\",\"%.2f\"%(hits),\"% hits\",\"and\",\"%.2f\"%(faults),\"% faults\")\nprint(\"Total samples of test\",n_samples)\nprint(\"*Iris-Setosa:\",n_set,\"samples\")\nprint(\"*Iris-Versicolour:\",n_vers,\"samples\")\nprint(\"*Iris-Virginica:\",n_virg,\"samples\")\n\ngraph_hits.append(hits)\ngraph_hits.append(faults)\nlabels = 'Hits', 'Faults';\nsizes = [96.5, 3.3]\nexplode = (0, 0.14)\n\nfig1, ax1 = plt.subplots();\nax1.pie(graph_hits, explode=explode,colors=['blue','red'],labels=labels, autopct='%1.1f%%',\nshadow=True, startangle=90)\nax1.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4. Score for each one of the samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_set = (score_set/n_set)*100\nacc_vers = (score_vers/n_vers)*100\nacc_virg = (score_virg/n_virg)*100\nprint(\"- Acurracy Iris-Setosa:\",\"%.2f\"%acc_set, \"%\")\nprint(\"- Acurracy Iris-Versicolour:\",\"%.2f\"%acc_vers, \"%\")\nprint(\"- Acurracy Iris-Virginica:\",\"%.2f\"%acc_virg, \"%\")\nnames = [\"Setosa\",\"Versicolour\",\"Virginica\"]\nx1 = [2.0,4.0,6.0]\nfig, ax = plt.subplots()\nr1 = plt.bar(x1[0], acc_set,color='orange',label='Iris-Setosa')\nr2 = plt.bar(x1[1], acc_vers,color='green',label='Iris-Versicolour')\nr3 = plt.bar(x1[2], acc_virg,color='purple',label='Iris-Virginica')\nplt.ylabel('Scores %')\nplt.xticks(x1, names);plt.title('Scores by iris flowers - Multilayer Perceptron')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}